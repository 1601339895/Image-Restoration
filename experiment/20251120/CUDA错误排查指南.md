# CUDA Kernelé”™è¯¯æ’æŸ¥æŒ‡å—

## ğŸ› é—®é¢˜æè¿°

CUDA kernelé”™è¯¯é€šå¸¸è¡¨ç°ä¸ºï¼š
```
CUDA kernel errors might be asynchronously reported at some other API call,
so the stacktrace below might be incorrect.
```

è¿™æ˜¯ç”±äºCUDAæ“ä½œæ˜¯å¼‚æ­¥çš„ï¼Œé”™è¯¯å¯èƒ½åœ¨å®é™…å‘ç”Ÿä½ç½®ä¹‹åæ‰è¢«æŠ¥å‘Šã€‚

---

## âœ… å·²ä¿®å¤çš„é—®é¢˜

### é—®é¢˜1ï¼šé¢‘åŸŸåˆ†æ”¯ç»´åº¦ä¸åŒ¹é…

**åŸå› ï¼š**
```python
# é”™è¯¯ç¤ºä¾‹ï¼š
fft_mag = torch.abs(fft)  # (B, C/2, H, W)
fft_phase = torch.angle(fft)  # (B, C/2, H, W)
fft_mag_phase = torch.cat([fft_mag, fft_phase], dim=1)  # (B, C, H, W) âœ… æ­£ç¡®

# ä½†åç»­å¤„ç†æœŸæœ›çš„ç»´åº¦ä¸å®é™…ä¸ç¬¦
freq_weights = self.freq_gate(fft_feat)  # æœŸæœ›è¾“å‡º (B, C, 1, 1)
```

**ä¿®å¤æ–¹æ¡ˆï¼š**

å·²åœ¨ä»£ç ä¸­æ·»åŠ è¯¦ç»†çš„ç»´åº¦æ³¨é‡Šï¼Œç¡®ä¿æ•°æ®æµæ¸…æ™°ï¼š

```python
# ç¬¬410-436è¡Œï¼ˆFrequencyAwareBlock.forwardï¼‰
fft = torch.fft.fft2(freq_x, dim=(-2, -1))  # freq_x: (B, C/2, H, W)
fft_mag = torch.abs(fft)  # (B, C/2, H, W)
fft_phase = torch.angle(fft)  # (B, C/2, H, W)
fft_mag_phase = torch.cat([fft_mag, fft_phase], dim=1)  # (B, C, H, W) - æ‹¼æ¥åé€šé“ç¿»å€
fft_feat = self.fft_mag_phase_extract(fft_mag_phase)  # (B, C/2, H, W) - 1x1å·ç§¯é™ç»´
freq_weights = self.freq_gate(fft_feat)  # (B, C, 1, 1) - Cé€šé“ç”¨äºåˆ†æˆlow/highå„C/2
low_freq_weight, high_freq_weight = freq_weights.chunk(2, dim=1)  # å„(B, C/2, 1, 1)
```

---

## ğŸ” è°ƒè¯•æ–¹æ³•

### æ–¹æ³•1ï¼šå¯ç”¨åŒæ­¥CUDAæ‰§è¡Œï¼ˆæ¨èï¼‰

åœ¨è¿è¡Œä»£ç å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼š

```bash
# Windows (PowerShell)
$env:CUDA_LAUNCH_BLOCKING=1
python FPN_Restormer_CA_CNN_Encoder.py

# Windows (CMD)
set CUDA_LAUNCH_BLOCKING=1
python FPN_Restormer_CA_CNN_Encoder.py

# Linux/Mac
export CUDA_LAUNCH_BLOCKING=1
python FPN_Restormer_CA_CNN_Encoder.py
```

è¿™ä¼šè®©CUDAæ“ä½œåŒæ­¥æ‰§è¡Œï¼Œé”™è¯¯å †æ ˆä¼šæŒ‡å‘å®é™…å‡ºé”™ä½ç½®ã€‚

### æ–¹æ³•2ï¼šé€æ¨¡å—æµ‹è¯•

åˆ›å»ºæµ‹è¯•è„šæœ¬ `test_modules.py`ï¼š

```python
import torch
import torch.nn as nn
from FPN_Restormer_CA_CNN_Encoder import FrequencyAwareBlock, Light_FFT_DSConv_Block

device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# æµ‹è¯•1ï¼šåŸå§‹Block
print("\næµ‹è¯•1: Light_FFT_DSConv_Block")
try:
    block_original = Light_FFT_DSConv_Block(dim=96, bias=False, dilation_rate=1).to(device)
    x = torch.randn(2, 96, 64, 64).to(device)
    out = block_original(x)
    print(f"âœ… æˆåŠŸï¼è¾“å…¥: {x.shape}, è¾“å‡º: {out.shape}")
except Exception as e:
    print(f"âŒ å¤±è´¥ï¼š{e}")

# æµ‹è¯•2ï¼šFrequencyAwareBlock
print("\næµ‹è¯•2: FrequencyAwareBlock")
try:
    block_freq = FrequencyAwareBlock(dim=96, bias=False, dilation_rate=1).to(device)
    x = torch.randn(2, 96, 64, 64).to(device)

    # é€æ­¥æµ‹è¯•
    print("æ­¥éª¤1: é€šé“æ‹†åˆ†...")
    spatial_x, freq_x = x.chunk(2, dim=1)
    print(f"  spatial_x: {spatial_x.shape}, freq_x: {freq_x.shape}")

    print("æ­¥éª¤2: FFTå˜æ¢...")
    fft = torch.fft.fft2(freq_x, dim=(-2, -1))
    print(f"  fft: {fft.shape}")

    print("æ­¥éª¤3: å¹…åº¦å’Œç›¸ä½æå–...")
    fft_mag = torch.abs(fft)
    fft_phase = torch.angle(fft)
    print(f"  fft_mag: {fft_mag.shape}, fft_phase: {fft_phase.shape}")

    print("æ­¥éª¤4: æ‹¼æ¥...")
    fft_mag_phase = torch.cat([fft_mag, fft_phase], dim=1)
    print(f"  fft_mag_phase: {fft_mag_phase.shape}")

    print("æ­¥éª¤5: å®Œæ•´å‰å‘ä¼ æ’­...")
    out = block_freq(x)
    print(f"âœ… æˆåŠŸï¼è¾“å…¥: {x.shape}, è¾“å‡º: {out.shape}")
except Exception as e:
    print(f"âŒ å¤±è´¥ï¼š{e}")
    import traceback
    traceback.print_exc()

# æµ‹è¯•3ï¼šä¸åŒç»´åº¦æµ‹è¯•
print("\næµ‹è¯•3: ä¸åŒç»´åº¦")
test_dims = [48, 96, 192, 384]  # å¯¹åº”level1, level2, level3, latent
for dim in test_dims:
    try:
        block = FrequencyAwareBlock(dim=dim, bias=False, dilation_rate=2).to(device)
        x = torch.randn(1, dim, 32, 32).to(device)
        out = block(x)
        print(f"âœ… dim={dim}: è¾“å…¥{x.shape} -> è¾“å‡º{out.shape}")
    except Exception as e:
        print(f"âŒ dim={dim} å¤±è´¥: {e}")

print("\næ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
```

è¿è¡Œæµ‹è¯•ï¼š
```bash
CUDA_LAUNCH_BLOCKING=1 python test_modules.py
```

### æ–¹æ³•3ï¼šæ£€æŸ¥CUDAå†…å­˜

å¦‚æœæ˜¯æ˜¾å­˜ä¸è¶³å¯¼è‡´çš„é”™è¯¯ï¼š

```python
import torch

def print_gpu_memory():
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated() / 1024**3
        reserved = torch.cuda.memory_reserved() / 1024**3
        print(f"GPUæ˜¾å­˜ - å·²åˆ†é…: {allocated:.2f}GB, å·²é¢„ç•™: {reserved:.2f}GB")

# åœ¨æ¨¡å‹å‰å‘ä¼ æ’­å‰åè°ƒç”¨
print_gpu_memory()
out = model(inp)
print_gpu_memory()
```

---

## ğŸ”§ å¸¸è§CUDAé”™è¯¯åŸå› åŠè§£å†³æ–¹æ¡ˆ

### 1. ç»´åº¦ä¸åŒ¹é…ï¼ˆæœ€å¸¸è§ï¼‰

**ç—‡çŠ¶ï¼š** RuntimeError: mat1 and mat2 shapes cannot be multiplied

**æ£€æŸ¥ï¼š**
```python
# åœ¨FrequencyAwareBlock.forwardä¸­æ·»åŠ æ–­ç‚¹
print(f"spatial_x: {spatial_x.shape}")
print(f"freq_x: {freq_x.shape}")
print(f"fft_mag_phase: {fft_mag_phase.shape}")
print(f"fft_feat: {fft_feat.shape}")
```

**é¢„æœŸè¾“å‡ºï¼š**
```
spatial_x: torch.Size([B, C/2, H, W])
freq_x: torch.Size([B, C/2, H, W])
fft_mag_phase: torch.Size([B, C, H, W])  # C/2 * 2 = C
fft_feat: torch.Size([B, C/2, H, W])     # 1x1å·ç§¯é™ç»´
```

### 2. æ•°å€¼ç¨³å®šæ€§é—®é¢˜

**ç—‡çŠ¶ï¼š** CUDA error: an illegal memory access was encountered

**åŸå› ï¼š** FFTåçš„ç›¸ä½è§’åº¦å¯èƒ½åŒ…å«NaNæˆ–Inf

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# åœ¨ç¬¬416è¡Œåæ·»åŠ æ•°å€¼æ£€æŸ¥
fft_phase = torch.angle(fft)
if torch.isnan(fft_phase).any() or torch.isinf(fft_phase).any():
    print("è­¦å‘Šï¼šfft_phaseåŒ…å«NaNæˆ–Infï¼Œè¿›è¡Œè£å‰ª")
    fft_phase = torch.clamp(fft_phase, -3.14159, 3.14159)
```

### 3. æ˜¾å­˜ä¸è¶³

**ç—‡çŠ¶ï¼š** CUDA out of memory

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# æ–¹æ³•1ï¼šå‡å°batch sizeæˆ–å›¾åƒå°ºå¯¸
inp = torch.randn(1, 3, 128, 128).cuda()  # ä»224é™åˆ°128

# æ–¹æ³•2ï¼šä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼ˆè®­ç»ƒæ—¶ï¼‰
import torch.utils.checkpoint as checkpoint
out = checkpoint.checkpoint(model.encoder_level1, x)

# æ–¹æ³•3ï¼šå‡å°æ¨¡å‹ç»´åº¦
model = Restormer_FFT_DSConv_Fusion(
    dim=40,  # ä»48é™åˆ°40
    use_frequency_aware=True
)
```

### 4. CUDAç‰ˆæœ¬ä¸å…¼å®¹

**æ£€æŸ¥ï¼š**
```python
import torch
print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
print(f"cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
```

**è¦æ±‚ï¼š**
- PyTorch >= 1.8.0ï¼ˆæ”¯æŒtorch.fft.fft2ï¼‰
- CUDA >= 10.2
- cuDNN >= 7.6

---

## ğŸ¯ å¿«é€Ÿæ’æŸ¥æµç¨‹

### Step 1: ç¡®è®¤ç¯å¢ƒ
```bash
python -c "import torch; print('PyTorch:', torch.__version__); print('CUDA:', torch.cuda.is_available())"
```

### Step 2: CPUæ¨¡å¼æµ‹è¯•
```python
# åœ¨ä¸»ç¨‹åºä¸­ä¿®æ”¹
device = 'cpu'  # å¼ºåˆ¶ä½¿ç”¨CPU
inp = torch.randn(1, 3, 224, 224).to(device)
model = model.to(device)
```

å¦‚æœCPUæ¨¡å¼æ­£å¸¸ï¼Œè¯´æ˜æ˜¯CUDAç‰¹å®šé—®é¢˜ã€‚

### Step 3: é€å±‚æµ‹è¯•
```python
# æµ‹è¯•ç¼–ç å™¨å„å±‚
print("æµ‹è¯•level1...")
out1 = model.encoder_level1(model.patch_embed(inp))
print(f"level1è¾“å‡º: {out1.shape}")

print("æµ‹è¯•level2...")
out2 = model.encoder_level2(model.down1_2(out1))
print(f"level2è¾“å‡º: {out2.shape}")

# ä¾æ­¤ç±»æ¨...
```

### Step 4: å¯¹æ¯”åŸå§‹Block
```python
# å¦‚æœFrequencyAwareBlockæŠ¥é”™ï¼Œåˆ‡æ¢å›åŸå§‹Block
model = Restormer_FFT_DSConv_Fusion(
    use_frequency_aware=False,  # ä½¿ç”¨åŸå§‹Light_FFT_DSConv_Block
    task_aware_fusion=False
)
```

---

## ğŸ“ ä¿®å¤æ—¥å¿—

### 2025-01-20 ä¿®å¤è®°å½•

**é—®é¢˜ï¼š** FrequencyAwareBlockä¸­é¢‘åŸŸåˆ†æ”¯ç»´åº¦ä¸æ¸…æ™°

**ä¿®å¤ï¼š**
1. æ·»åŠ è¯¦ç»†çš„ç»´åº¦æ³¨é‡Šï¼ˆç¬¬414-436è¡Œï¼‰
2. æ˜ç¡®å„æ­¥éª¤çš„å¼ é‡å½¢çŠ¶
3. ç¡®ä¿freq_gateè¾“å‡ºé€šé“æ•°æ­£ç¡®ï¼ˆCé€šé“ï¼Œç”¨äºåˆ†æˆlow/highå„C/2ï¼‰

**éªŒè¯ï¼š**
```python
# æµ‹è¯•ç”¨ä¾‹
block = FrequencyAwareBlock(dim=96, bias=False, dilation_rate=1)
x = torch.randn(2, 96, 64, 64)
out = block(x)
assert out.shape == x.shape, "è¾“å‡ºå½¢çŠ¶åº”ä¸è¾“å…¥ç›¸åŒ"
print("âœ… ç»´åº¦æµ‹è¯•é€šè¿‡")
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. å¼€å‘æ—¶å»ºè®®

```python
# å¼€å¯è°ƒè¯•æ¨¡å¼
torch.autograd.set_detect_anomaly(True)

# è®¾ç½®CUDAåŒæ­¥
torch.cuda.synchronize()

# æ·»åŠ ç»´åº¦æ–­è¨€
def forward(self, x):
    assert x.dim() == 4, f"æœŸæœ›4Då¼ é‡ï¼Œå¾—åˆ°{x.dim()}D"
    assert x.size(1) == self.dim, f"æœŸæœ›{self.dim}é€šé“ï¼Œå¾—åˆ°{x.size(1)}"
    # ... æ­£å¸¸å‰å‘ä¼ æ’­
```

### 2. ç”Ÿäº§ç¯å¢ƒå»ºè®®

```python
# å…³é—­è°ƒè¯•ï¼ˆæå‡é€Ÿåº¦ï¼‰
torch.autograd.set_detect_anomaly(False)

# ä½¿ç”¨æ··åˆç²¾åº¦ï¼ˆèŠ‚çœæ˜¾å­˜ï¼‰
from torch.cuda.amp import autocast
with autocast():
    out = model(inp)

# ä½¿ç”¨ç¼–è¯‘ä¼˜åŒ–ï¼ˆPyTorch 2.0+ï¼‰
model = torch.compile(model)
```

---

## ğŸ“§ åé¦ˆ

å¦‚æœä»¥ä¸Šæ–¹æ³•ä»æ— æ³•è§£å†³é—®é¢˜ï¼Œè¯·æä¾›ä»¥ä¸‹ä¿¡æ¯ï¼š

1. **å®Œæ•´é”™è¯¯å †æ ˆ**ï¼ˆä½¿ç”¨CUDA_LAUNCH_BLOCKING=1è¿è¡Œï¼‰
2. **ç¯å¢ƒä¿¡æ¯**ï¼š
   ```python
   import torch
   print(torch.__version__)
   print(torch.version.cuda)
   print(torch.cuda.get_device_name(0))
   ```
3. **è¾“å…¥æ•°æ®å½¢çŠ¶**ï¼š
   ```python
   print(f"è¾“å…¥: {inp.shape}")
   ```
4. **æ¨¡å‹é…ç½®**ï¼š
   ```python
   print(f"dim={dim}, use_frequency_aware={use_frequency_aware}")
   ```

---

## âœ… ç¡®è®¤ä¿®å¤

è¿è¡Œä»¥ä¸‹å‘½ä»¤ç¡®è®¤é—®é¢˜å·²è§£å†³ï¼š

```bash
CUDA_LAUNCH_BLOCKING=1 python FPN_Restormer_CA_CNN_Encoder.py
```

é¢„æœŸè¾“å‡ºï¼š
```
================================================================================
æ¨¡å‹å¯¹æ¯”å®éªŒï¼šFrequencyAwareBlock vs åŸå§‹Light_FFT_DSConv_Block
================================================================================

================================================================================
ã€æµ‹è¯•1ã€‘åŸå§‹æ¨¡å‹ (Light_FFT_DSConv_Block)
================================================================================
è¾“å‡ºå½¢çŠ¶: torch.Size([1, 3, 224, 224])
å‚æ•°é‡: 11.817M
æ˜¾å­˜å ç”¨: 8188.808 MB
...
```

å¦‚æœæ­£å¸¸è¾“å‡ºï¼Œè¯´æ˜CUDAé”™è¯¯å·²ä¿®å¤ï¼ğŸ‰
