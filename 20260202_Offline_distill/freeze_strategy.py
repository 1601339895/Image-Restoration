import torch
import numpy as np
from collections import defaultdict
from typing import Dict, List, Tuple, Any, Optional


class FreezeStrategy:
    """é«˜çº§å†»ç»“ç­–ç•¥ç®¡ç†å™¨ï¼Œæ”¯æŒå¤šç§å†»ç»“ç­–ç•¥"""
    
    def __init__(self, model: torch.nn.Module, config: Dict, logger):
        self.model = model
        self.config = config
        self.logger = logger
        self.strategy = config.get('strategy', 'freeze_all_except_last_layers')
        self.frozen_layers = config.get('frozen_layers', [])
        self.freeze_ratio = config.get('freeze_ratio', 0.8)
        self.layer_mapping = self._create_layer_mapping()
        self.initial_state = {}
        self._save_initial_state()
    
    def _create_layer_mapping(self) -> Dict[str, torch.nn.Module]:
        """åˆ›å»ºå±‚åç§°åˆ°æ¨¡å—çš„æ˜ å°„ï¼Œæ”¯æŒInfNetæ¶æ„"""
        layer_mapping = {}
        
        # Steméƒ¨åˆ†
        layer_mapping['stem'] = torch.nn.Sequential(
            self.model.conv1,
            self.model.bn1,
            self.model.stem_dw
        )
        
        # ResNetå±‚
        layer_mapping['layer1'] = self.model.layer1
        layer_mapping['layer2'] = self.model.layer2
        layer_mapping['layer3'] = self.model.layer3
        layer_mapping['layer4'] = self.model.layer4
        
        # Headéƒ¨åˆ†
        if hasattr(self.model, 'head_conv') and hasattr(self.model, 'fc'):
            layer_mapping['head'] = torch.nn.Sequential(
                self.model.head_conv,
                self.model.head_bn,
                self.model.fc
            )
        elif hasattr(self.model, 'global_pool') and hasattr(self.model, 'fc'):
            layer_mapping['head'] = torch.nn.Sequential(
                self.model.global_pool,
                self.model.dropout,
                self.model.fc
            )
        
        return layer_mapping
    
    def _save_initial_state(self):
        """ä¿å­˜åˆå§‹å‚æ•°çŠ¶æ€ç”¨äºéªŒè¯"""
        for name, param in self.model.named_parameters():
            self.initial_state[name] = param.data.clone().cpu()
    
    def apply_freeze(self):
        """åº”ç”¨å†»ç»“ç­–ç•¥"""
        self.logger(f"ğŸ”’ Applying freeze strategy: {self.strategy}")
        
        if self.strategy == "layer_wise":
            self._freeze_layer_wise()
        elif self.strategy == "ratio":
            self._freeze_by_ratio()
        elif self.strategy == "freeze_all_except_head":
            self._freeze_all_except_head()
        elif self.strategy == "freeze_all_except_last_layers":
            self._freeze_all_except_last_layers()
        else:
            raise ValueError(f"Unknown freeze strategy: {self.strategy}")
        
        # éªŒè¯å†»ç»“ç»“æœ
        self._verify_freeze()
    
    def _freeze_layer_wise(self):
        """æŒ‰å±‚åç§°å†»ç»“æŒ‡å®šå±‚"""
        for layer_name in self.frozen_layers:
            if layer_name in self.layer_mapping:
                module = self.layer_mapping[layer_name]
                self._freeze_module(module, layer_name)
                self.logger(f"  âœ… Frozen layer: {layer_name}")
            else:
                self.logger(f"  âš ï¸ Layer not found: {layer_name}")
    
    def _freeze_by_ratio(self):
        """æŒ‰å‚æ•°é‡æ¯”ä¾‹å†»ç»“"""
        # è·å–æ‰€æœ‰å‚æ•°
        all_params = [(name, param) for name, param in self.model.named_parameters()]
        
        # æŒ‰å‚æ•°åœ¨æ¨¡å‹ä¸­çš„é¡ºåºæ’åºï¼ˆé€šå¸¸æ˜¯ä»è¾“å…¥åˆ°è¾“å‡ºï¼‰
        all_params.sort(key=lambda x: x[0])
        
        # è®¡ç®—æ€»å‚æ•°é‡
        total_params = sum(p.numel() for _, p in all_params)
        freeze_threshold = int(total_params * self.freeze_ratio)
        
        self.logger(f"  Total parameters: {total_params:,}")
        self.logger(f"  Freezing first {self.freeze_ratio*100:.1f}% ({freeze_threshold:,} params)")
        
        # å†»ç»“å‚æ•°
        accumulated_params = 0
        for name, param in all_params:
            if accumulated_params < freeze_threshold:
                param.requires_grad = False
                accumulated_params += param.numel()
            else:
                param.requires_grad = True
        
        self.logger(f"  âœ… Frozen {accumulated_params:,} parameters ({accumulated_params/total_params:.1%})")
    
    def _freeze_all_except_head(self):
        """å†»ç»“é™¤äº†headå’Œfcä¹‹å¤–çš„æ‰€æœ‰å±‚"""
        for layer_name, module in self.layer_mapping.items():
            if layer_name != 'head':
                self._freeze_module(module, layer_name)
                self.logger(f"  âœ… Frozen layer: {layer_name}")
    
    def _freeze_all_except_last_layers(self):
        """å†»ç»“é™¤äº†layer4å’Œheadä¹‹å¤–çš„æ‰€æœ‰å±‚ï¼ˆæœ€é€‚åˆå¤´ç›”åœºæ™¯ï¼‰"""
        layers_to_keep_trainable = ['layer4', 'head']
        
        for layer_name, module in self.layer_mapping.items():
            if layer_name not in layers_to_keep_trainable:
                self._freeze_module(module, layer_name)
                self.logger(f"  âœ… Frozen layer: {layer_name}")
            else:
                self.logger(f"  ğŸ¯ Keeping layer trainable: {layer_name}")
    
    def _freeze_module(self, module: torch.nn.Module, layer_name: str):
        """å†»ç»“æ•´ä¸ªæ¨¡å—åŠå…¶å­æ¨¡å—"""
        for name, param in module.named_parameters():
            param.requires_grad = False
    
    def _verify_freeze(self):
        """éªŒè¯å†»ç»“ç»“æœ"""
        total_params = 0
        frozen_params = 0
        layer_status = defaultdict(lambda: {'param_count': 0, 'frozen_count': 0})
        
        for name, param in self.model.named_parameters():
            param_count = param.numel()
            total_params += param_count
            
            # ç¡®å®šå‚æ•°æ‰€å±å±‚
            layer_name = 'unknown'
            if 'conv1' in name or 'bn1' in name or 'stem_dw' in name:
                layer_name = 'stem'
            elif 'layer1' in name:
                layer_name = 'layer1'
            elif 'layer2' in name:
                layer_name = 'layer2'
            elif 'layer3' in name:
                layer_name = 'layer3'
            elif 'layer4' in name:
                layer_name = 'layer4'
            else:
                layer_name = 'head'
            
            layer_status[layer_name]['param_count'] += param_count
            if not param.requires_grad:
                frozen_params += param_count
                layer_status[layer_name]['frozen_count'] += param_count
        
        frozen_ratio = frozen_params / total_params if total_params > 0 else 0
        self.logger(f"ğŸ“Š Freeze Verification:")
        self.logger(f"   Total parameters: {total_params:,}")
        self.logger(f"   Frozen parameters: {frozen_params:,} ({frozen_ratio:.1%})")
        self.logger(f"   Trainable parameters: {total_params - frozen_params:,} ({1 - frozen_ratio:.1%})")
        
        # æŒ‰å±‚æ˜¾ç¤ºå†»ç»“çŠ¶æ€
        self.logger("   Layer-wise status:")
        for layer_name, status in layer_status.items():
            layer_frozen_ratio = status['frozen_count'] / status['param_count'] if status['param_count'] > 0 else 0
            status_str = "ğŸ”’ FROZEN" if layer_frozen_ratio > 0.99 else "ğŸ¯ TRAINABLE"
            self.logger(f"      {layer_name}: {status_str} ({status['frozen_count']:,}/{status['param_count']:,} frozen)")
    
    def validate_pretrained_loading(self):
        """éªŒè¯é¢„è®­ç»ƒæƒé‡æ˜¯å¦æ­£ç¡®åŠ è½½"""
        self.logger("ğŸ” Validating pretrained weight loading...")
        
        mismatched_params = []
        for name, param in self.model.named_parameters():
            if name in self.initial_state:
                initial = self.initial_state[name]
                current = param.data.cpu()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰å˜åŒ–ï¼ˆæ’é™¤éšæœºåˆå§‹åŒ–çš„å°å·®å¼‚ï¼‰
                diff = torch.abs(initial - current).mean().item()
                if diff > 1e-6:  # é˜ˆå€¼ï¼Œè€ƒè™‘æµ®ç‚¹ç²¾åº¦
                    mismatched_params.append((name, diff))
        
        if mismatched_params:
            self.logger(f"   âš ï¸ {len(mismatched_params)} parameters changed after loading:")
            for name, diff in mismatched_params[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                self.logger(f"      {name}: diff={diff:.6f}")
            if len(mismatched_params) > 10:
                self.logger(f"      ... and {len(mismatched_params)-10} more")
        else:
            self.logger("   âœ… All parameters match initial state - weights loaded correctly!")
    
    def get_freeze_info(self) -> Dict[str, Any]:
        """è·å–å†»ç»“ä¿¡æ¯ç”¨äºæ—¥å¿—å’Œç›‘æ§"""
        total_params = 0
        frozen_params = 0
        layer_status = {}
        
        for name, param in self.model.named_parameters():
            param_count = param.numel()
            total_params += param_count
            
            # ç¡®å®šå‚æ•°æ‰€å±å±‚
            layer_name = 'unknown'
            if 'conv1' in name or 'bn1' in name or 'stem_dw' in name:
                layer_name = 'stem'
            elif 'layer1' in name:
                layer_name = 'layer1'
            elif 'layer2' in name:
                layer_name = 'layer2'
            elif 'layer3' in name:
                layer_name = 'layer3'
            elif 'layer4' in name:
                layer_name = 'layer4'
            else:
                layer_name = 'head'
            
            if layer_name not in layer_status:
                layer_status[layer_name] = {'param_count': 0, 'frozen_count': 0, 'frozen': False}
            
            layer_status[layer_name]['param_count'] += param_count
            if not param.requires_grad:
                frozen_params += param_count
                layer_status[layer_name]['frozen_count'] += param_count
        
        # ç¡®å®šæ¯å±‚æ˜¯å¦å†»ç»“
        for layer_name, status in layer_status.items():
            status['frozen'] = (status['frozen_count'] / status['param_count']) > 0.99
        
        frozen_ratio = frozen_params / total_params if total_params > 0 else 0
        trainable_ratio = 1 - frozen_ratio
        
        return {
            'frozen_ratio': frozen_ratio,
            'trainable_ratio': trainable_ratio,
            'frozen_count': frozen_params,
            'trainable_count': total_params - frozen_params,
            'total_count': total_params,
            'layer_status': layer_status,
            'strategy': self.strategy
        }


class GradientMonitor:
    """æ¢¯åº¦ç›‘æ§å™¨ï¼Œç”¨äºç›‘æ§å†»ç»“å±‚çš„æ¢¯åº¦æµåŠ¨"""
    
    def __init__(self, model: torch.nn.Module, logger):
        self.model = model
        self.logger = logger
        self.grad_stats = defaultdict(lambda: {'count': 0, 'grad_norm_sum': 0.0})
        self.last_log_step = 0
        self.config = {
            'enable_grad_monitor': True,
            'log_interval': 50,
            'detect_grad_leak': True
        }
    
    def monitor_gradients(self, current_step: int, tb_logger=None):
        """ç›‘æ§æ¢¯åº¦ï¼Œæ£€æµ‹å†»ç»“å±‚çš„æ¢¯åº¦æ³„éœ²"""
        if not self.config['enable_grad_monitor']:
            return
        
        has_grad_leak = False
        grad_leak_details = []
        
        for name, param in self.model.named_parameters():
            if param.grad is not None:
                grad_norm = torch.norm(param.grad).item()
                self.grad_stats[name]['count'] += 1
                self.grad_stats[name]['grad_norm_sum'] += grad_norm
                
                # æ£€æµ‹å†»ç»“å±‚çš„æ¢¯åº¦æ³„éœ²
                if not param.requires_grad and grad_norm > 1e-8:  # éé›¶æ¢¯åº¦
                    has_grad_leak = True
                    grad_leak_details.append((name, grad_norm))
        
        # å®šæœŸè®°å½•æ¢¯åº¦ç»Ÿè®¡
        if current_step - self.last_log_step >= self.config['log_interval']:
            self._log_gradient_stats(current_step, tb_logger)
            
            # æ£€æµ‹æ¢¯åº¦æ³„éœ²
            if has_grad_leak and self.config['detect_grad_leak']:
                self._log_grad_leak(current_step, grad_leak_details)
            
            self.last_log_step = current_step
    
    def _log_gradient_stats(self, step: int, tb_logger=None):
        """è®°å½•æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯"""
        self.logger(f"ğŸ“ˆ Gradient stats at step {step}:")
        
        # æŒ‰å±‚åˆ†ç»„ç»Ÿè®¡
        layer_stats = defaultdict(lambda: {'param_count': 0, 'grad_norm_sum': 0.0, 'grad_count': 0})
        
        for name, stats in self.grad_stats.items():
            if stats['count'] > 0:
                avg_grad_norm = stats['grad_norm_sum'] / stats['count']
                
                # ç¡®å®šå‚æ•°æ‰€å±å±‚
                layer_name = 'unknown'
                if 'conv1' in name or 'bn1' in name or 'stem_dw' in name:
                    layer_name = 'stem'
                elif 'layer1' in name:
                    layer_name = 'layer1'
                elif 'layer2' in name:
                    layer_name = 'layer2'
                elif 'layer3' in name:
                    layer_name = 'layer3'
                elif 'layer4' in name:
                    layer_name = 'layer4'
                else:
                    layer_name = 'head'
                
                layer_stats[layer_name]['grad_norm_sum'] += avg_grad_norm
                layer_stats[layer_name]['grad_count'] += 1
        
        # è®°å½•æ¯å±‚å¹³å‡æ¢¯åº¦
        for layer_name, stats in layer_stats.items():
            if stats['grad_count'] > 0:
                avg_layer_grad = stats['grad_norm_sum'] / stats['grad_count']
                log_msg = f"   {layer_name}: avg_grad_norm={avg_layer_grad:.6f}"
                
                # ç‰¹åˆ«æ ‡è®°å†»ç»“å±‚çš„æ¢¯åº¦
                if 'stem' in layer_name or 'layer1' in layer_name or 'layer2' in layer_name or 'layer3' in layer_name:
                    if avg_layer_grad > 1e-8:
                        log_msg += " âš ï¸(unexpected grad)"
                
                self.logger(log_msg)
                
                # TensorBoardè®°å½•
                if tb_logger is not None:
                    tb_logger.write(f"grad_norm/{layer_name}", avg_layer_grad, step)
        
        # é‡ç½®ç»Ÿè®¡
        self.grad_stats.clear()
    
    def _log_grad_leak(self, step: int, leak_details: List[Tuple[str, float]]):
        """è®°å½•æ¢¯åº¦æ³„éœ²è¯¦æƒ…"""
        self.logger(f"ğŸš¨ GRADIENT LEAK DETECTED at step {step}!")
        self.logger(f"   {len(leak_details)} frozen parameters received non-zero gradients:")
        
        # æŒ‰æ¢¯åº¦å¤§å°æ’åºï¼Œæ˜¾ç¤ºæœ€ä¸¥é‡çš„
        leak_details.sort(key=lambda x: x[1], reverse=True)
        for name, grad_norm in leak_details[:10]:
            self.logger(f"      {name}: grad_norm={grad_norm:.6f}")
        
        if len(leak_details) > 10:
            self.logger(f"      ... and {len(leak_details)-10} more")
        
        # å»ºè®®ä¿®å¤æªæ–½
        self.logger("   ğŸ’¡ Suggested fixes:")
        self.logger("      1. Check if DDP find_unused_parameters=True is causing this")
        self.logger("      2. Verify that frozen parameters are not used in loss computation")
        self.logger("      3. Consider using torch.no_grad() context for frozen parts")